 Lecture 4: Backpropagation

**Recap**

Storing a 'local' error, i.e., the common terms in the backpropagation, is useful in computation.
'When we differentiate wrt one element of the matrix W(i, j), we get x(j). 

**Tips to Derive Gradients**

1. Define variables carefully and keep track of their dimension.
2. Be careful while using the chain rule.
3. While using softmax, take derivatives separately for the correct and incorrect class.





<p align="center">
  <img width="550" height="350" src="https://user-images.githubusercontent.com/21968647/64905019-261eea00-d687-11e9-96ae-0c55b4c38da0.png">
</p>

